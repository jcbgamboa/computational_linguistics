{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "=======\n",
    "\n",
    "From Wikipedia _(if you search for \"Tokenization\", you will fall into \"Word segmentation\")_:\n",
    "\n",
    ">_Word segmentation is the problem of dividing a string of written language into its component words._\n",
    "\n",
    "\n",
    "For example, our goal is to transform strings like\n",
    "\n",
    "    \"The woman drank her coffee\"\n",
    "\n",
    "into the list\n",
    "\n",
    "    \"The\", \"woman\", \"drank\", \"her\", \"coffee\"\n",
    "\n",
    "Notice that you definitely still want to keep the information about the order of the words. That is, for the example above, getting the list\n",
    "\n",
    "    \"woman\", \"coffee\", \"drank\", \"her\", \"the\"\n",
    "\n",
    "is definitely not a correct tokenization output.\n",
    "\n",
    "English is a \"well-behaved\" language in that it divides words mostly with spaces. For this\n",
    "class, we will only consider languages that do use spaces as delimiters of words. Tokenizing\n",
    "text without spaces in word boundaries is a hard problem and is briefly discussed in the\n",
    "end of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to make a distinction between _tokens_ and _types_. Consider the following sentence:\n",
    "\n",
    "    The woman saw another woman.\n",
    "\n",
    "Any normal human being would probably agree that this sentence has 5 words. However, what is the number of tokens in this sentence? The answer is 6: we should probably consider the period at the end as a token too.\n",
    "\n",
    "Now, what about the number of _types_ that this sentence has. What is a _type_? Well... a _type_ is each one of the _different_ tokens in the corpus. In the sentence above, the token _woman_ appears twice. Therefore, the number of types in it is 5: \"the\", \"woman\", \"saw\", \"another\", and \".\" (period).\n",
    "\n",
    "Now, what about the following sentence?\n",
    "\n",
    "    The woman saw the other woman.\n",
    "\n",
    "Here, again, the word _woman_ appears twice, so it should definitely be counted only once. But what about the word _the_? In the first case it appeared capital, but in the second case it appeared non-capital. Should these be counted separately? _(well... this is all debatable)_\n",
    "\n",
    "When talking about corpora, you might see descriptions saying that a corpus has (for example) 1 million _tokens_, but (for example) 80000 _types_. This is what it means.\n",
    "\n",
    "_As we learnt, a type that appears only once in a corpus is called a **Hapax Legomenon** (a Greek word for \"said only once\")._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you want to know about Tokenization?\n",
    "-------------------------------------------\n",
    "\n",
    "Tokenization is already implemented in many NLP libraries, like NLTK or spaCy, and most people could probably safely just take it for granted.\n",
    "\n",
    "However, it might be that certain algorithms require tokenization to be done in a certain way. For example, it might be beneficial sometimes to tokenize words like _wouldn't_ into _would_ and _n't_, but sometimes this might actually not matter. In this case, you are likely to want to implement your own tokenization. It shouldn't be a hard task, but it is useful to have a notion of what tools are there that can help us doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naïve approach\n",
    "----------------\n",
    "\n",
    "It seems from the example above that you could just separate the sentence by spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'woman', 'drank', 'her', 'coffee']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The woman drank her coffee\".split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this approach does not work well with punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Woman!', 'Drink', 'coffee...', 'Ok?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice how the punctuation now became a problem...\n",
    "\"Woman! Drink coffee... Ok?\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', \"woman's\", 'coffee', 'was', '(desperately,', 'quickly)', 'drunk.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or what about parenthesis? And commas?\n",
    "\"The woman's coffee was (desperately, quickly) drunk.\".split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach\n",
    "-------------------\n",
    "\n",
    "It looks like we could define some characters as punctuations. Say, we could decide to treat as a special token any of the following characters:\n",
    "\n",
    "    \".!?&,()[]/'\n",
    "\n",
    "For this kind of treatment, we will need some more powerful tools...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regular Expressions\n",
    "===========\n",
    "\n",
    "A Regular Expression (regex) is _\"a sequence of characters that define a search pattern. Usually this pattern is then used by string searching algorithms for \"find\" or \"find and replace\" operations on strings\"_. (Wikipedia)\n",
    "\n",
    "As the Wikipedia says, regexes look for patterns. These patterns can be as simple \"abc\" (that searches for the letter `a`, followed by the letter `b`, followed by the letter `c`), or as complicated as \"a sequence of three letters, followed by three numbers, followed by a slash, in the end of the line\". These are defined using a certain language, that, despite being a little hard to read, is very powerful for string manipulation.\n",
    "\n",
    "\n",
    "We will look into many more examples now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found abc\n"
     ]
    }
   ],
   "source": [
    "# Import the Python package for Regular Expressions\n",
    "import re\n",
    "\n",
    "# This Python package contains several utility functions for string manipulation with\n",
    "# Regular Expressions. One interesting function is `search(regex, str)`:\n",
    "if re.search('abc', 'my string has abc in it!'):\n",
    "    print(\"Found abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if re.search('abc', 'my string has abc in it!'):\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If your regex is complicated to read, you might want to take a look at\n",
    "[this website](https://regex101.com/) or\n",
    "[this website](https://regexr.com/). You might also find [this reference website](https://www.regular-expressions.info/) a good place to read more about them.\n",
    "\n",
    "Let's say that, instead of matching only the string `abc`, you would like to check if your string contains \"any number (including none) of letters `a`, followed by the letters `b` and `c`\". This could be done with the `*` operator. For example, this piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a*bc\n"
     ]
    }
   ],
   "source": [
    "my_string = 'my string has no letter a followed by bc'\n",
    "if re.search('a*bc', my_string):\n",
    "    print(\"Found a*bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a*bc\n"
     ]
    }
   ],
   "source": [
    "my_string = 'my string now is aaaaaaaaabc'\n",
    "if re.search('a*bc', my_string):\n",
    "    print(\"Found a*bc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will print `Found abc` twice: once because because there is zero `a`s before the letters `bc`, and another because there are many `a`s before the letters `bc`.\n",
    "\n",
    "Replacing\n",
    "-----------\n",
    "\n",
    "Regular Expressions are also useful for changing parts of a string. Let's say that the corpus you took your data from is full of errors, and you would like to fix some of them. Specifically, every time you find the word `the`, it contains several `t`s where it should contain only one. You now want to replace that group of \"any number of `t`s followed by a `h` and a `e`\" with the right word: `the`. Maybe you could try the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog chases the cat'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = 'tttttttthe dog chases the cat'\n",
    "\n",
    "# Replace is a function that receives three parameters:\n",
    "# 1) A regular expressions to be matched\n",
    "# 2) A string to be put in place of the matched regular expression\n",
    "# 3) The input line where the replacing will happen\n",
    "re.sub(\"t*he\", 'the', my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sub()` function will be used a lot throughout this text, and referring to each of its parameters might be a little confusing. Most of the time, I'll just say `regex` referring to the first parameter (which is actually what we are most interested in); but to avoid confusion, whenever I really need to be specific about the parameters I will try to refer to them by the following names:\n",
    "\n",
    " 1. **pattern**: the regular expression to be searched for in the _input string_\n",
    " 2. **replacement string**: the string to be put in the place of the _pattern_\n",
    " 3. **input string**: the string that contains the data you want to modify\n",
    "\n",
    "The problem with this code is that it will also match \"no `t`s\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the is hungry'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = 'he is hungry'\n",
    "re.sub(\"t*he\", 'the', my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English, the word `he` is a valid word, and you would like not to \"fix\" the words `he` too. There is a special regex operator for \"at least one\" occurrence of a letter: the `+` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is hungry\n",
      "the dog chases the cat\n"
     ]
    }
   ],
   "source": [
    "my_string = 'he is hungry'\n",
    "line = re.sub(\"t+he\", 'the', my_string)\n",
    "print(line)\n",
    "\n",
    "my_string = 'tttttttthe dog chases the cat'\n",
    "line = re.sub(\"t+he\", 'the', my_string)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Classes\n",
    "--------------------\n",
    "\n",
    "Let's say that your corpus has some other problem: because of the way it was extracted from the internet, it is composed by many characters that actually do not compose English words, like `#`, or `%` or even `@`. Let's say that you would like to remove them all. You could of course write a rule to replace each one of the characters by an empty string. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like coconut'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"I #like coco@nut%\"\n",
    "line = re.sub(\"#\", \"\", line)\n",
    "line = re.sub(\"%\", \"\", line)\n",
    "re.sub(\"@\", \"\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it would probably be tedious (and error prone) to have to create one line for each of the characters. Instead, regexes have a special syntax to allow you to match any character of a group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like coconut'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"I #like coco@nut%\"\n",
    "re.sub(\"[#@%]\", \"\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These square brackets are called [character classes](https://www.regular-expressions.info/refcharclass.html).\n",
    "\n",
    "Of course, you can always use the `*` and the `+` operator after the brackets to match multiple occurrences of the characters. For example, if you want to match any number of occurrences of the special characters followed by the word `nut`, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@##%@%# <-- this will not be matched; but _ this will'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"@##%@%# <-- this will not be matched; but @#%@#%nut this will\"\n",
    "re.sub(\"[@#%]*nut\", \"_\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our current solution for eliminating special characters is that we need to have a list of all special characters that we want to eliminate. We need to know that our corpus contains the character `#`, and then we need to insert it in our code line (i.e., in the text between the square brackets in our code).\n",
    "\n",
    "It would be nice if we could simply say what are the characters that we want, and then remove _any other character_. Regexes allow us to do that too. We can match any characters that are _not_ in a certain list by inserting a `^` as the first character inside the square brackets. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#@%'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"I #like coco@nut%\"\n",
    "\n",
    "# This will match only the characters that are NOT @, # or %,\n",
    "# and replace them with an empty string\n",
    "re.sub(\"[^@#%]\", \"\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some character classes are already \"predefined\" and you can just use them. For example, the class of all letters can be written as `\\w`, and the class of all non-letters can be written as `\\W` (notice that the W is capital here). Similarly, the class of all spaces is `\\s` and all the non-spaces are `\\S`. Finally, if you want to match just any character, you can always use `.`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:            <?#$> I like coconut .,[]{}/%!\n",
      "MATCHES ALL WORDS:   <?#$> _ ____ _______ .,[]{}/%!\n",
      "MATCHES NON-WORDS:   ______I_like_coconut__________\n",
      "MATCHES ALL SPACES:  <?#$>_I_like_coconut_.,[]{}/%!\n",
      "MATCHES NON-SPACES:  _____ _ ____ _______ _________\n",
      "MATCHES EVERYTHING:  ______________________________\n"
     ]
    }
   ],
   "source": [
    "line = \"<?#$> I like coconut .,[]{}/%!\"\n",
    "print(\"ORIGINAL:           \", line)\n",
    "\n",
    "# Replaces all the letters by \"_\"\n",
    "print(\"MATCHES ALL WORDS:  \", re.sub('\\w', '_', line))\n",
    "\n",
    "# Replaces all the non-letters by \"_\"\n",
    "print(\"MATCHES NON-WORDS:  \", re.sub('\\W', '_', line))\n",
    "\n",
    "# Replaces all the spaces by \"_\"\n",
    "print(\"MATCHES ALL SPACES: \", re.sub('\\s', '_', line))\n",
    "\n",
    "# Replaces all the non-spaces by \"_\"\n",
    "print(\"MATCHES NON-SPACES: \", re.sub('\\S', '_', line))\n",
    "\n",
    "# Replaces all the characters by \"_\"\n",
    "print(\"MATCHES EVERYTHING: \", re.sub('.', '_', line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupings\n",
    "-----------\n",
    "\n",
    "So far we have always been using regex to match either an entire string, or a set of characters, but never things like \"either `abc` or `def`\". Before, we had created a regex like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my string contains blah'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('abc', 'blah', 'my string contains abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now wanted to find \"either `abc` or `def` and replace both of the with `blah`, we can't use square brackets (i.e., `[ ]`), or otherwise we will match _each one of the characters inside the brachets_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my string blahontblahins blahblahblah'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[abcdef]', 'blah', 'my string contains abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right way to match multiple strings is by using parentheses (i.e., `(` and `)`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my string contains blah'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('(abc|def)', 'blah', 'my string contains def')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `|` operator works as an \"OR\" operator. That is, the regex above will match either `abc` or `def` and replace any of the two sequences as `blah`.\n",
    "\n",
    "Groupings are also useful if you want to swap things around. This is possible because it is possible to refer to each of the matched groups using a variable name. For example, let's say your corpus is composed of several sentences of the type\n",
    "\n",
    "    the dog chases a cat\n",
    "    the cat chases a mouse\n",
    "    the mouse eats the cheese\n",
    "    the human eats the beef\n",
    "\n",
    "And you'd like to swap the noun following `the` and the noun following `a`, so that you would end up getting something like\n",
    "\n",
    "    the cat chases a dog\n",
    "    the mouse chases a cat\n",
    "    the cheese eats the mouse\n",
    "    the beef eats the human\n",
    "\n",
    "Before solving this problem, remember that `(\\S)*` will match \"anything that is not a space\", which, if followed by a space, will probably capture a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each blah blah blah blah blah blah'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets each word followed by a space\n",
    "re.sub('\\s(\\S)*', ' blah', 'Each word here will become \" blah\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that you'll need to know is that groupings can be referred to using something called [Backreference](https://www.regular-expressions.info/backref.html). So, for example, if I wanted to swap the words `a` and the `the` of a sentence in my corpus (without knowing beforehand where the `a` and where the `the` is going to appear), I can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a cat and the dog'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('(the|a) (.*) (the|a)', '\\\\3 \\\\2 \\\\1', 'the cat and a dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot happening in this line. The first thing to notice is that this is the first time the replacement string (remember the names of the parameters of the `sub()` function above) has something \"special\" in it. Notice how there are three groups of parenthesis in the pattern, and how they correspond to the _backreferences_ `\\\\1`, `\\\\2` and `\\\\3` in the replacement string. Since we inverted the order of `\\\\3` and `\\\\1`, the `a` and the `the` were swapped in the output.\n",
    "\n",
    "We then have `(.*)`. As we discussed before, just the `.` matches any character, and putting `.*` matches `whatever number of \"any characters\"`. By putting this between parenthesis (i.e., in a _group_), we have a way to refer back to them in our replacement string.\n",
    "\n",
    "Now armed with this knowledge, we can fix our corpus with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat chases a dog\n",
      "the mouse chases a cat\n",
      "the cheese eats the mouse\n",
      "the beef eats the human\n"
     ]
    }
   ],
   "source": [
    "line1 = 'the dog chases a cat'\n",
    "line2 = 'the cat chases a mouse'\n",
    "line3 = 'the mouse eats the cheese'\n",
    "line4 = 'the human eats the beef'\n",
    "\n",
    "print(re.sub('(the|a) (\\S*) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line1))\n",
    "print(re.sub('(the|a) (\\S*) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line2))\n",
    "print(re.sub('(the|a) (\\S*) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line3))\n",
    "print(re.sub('(the|a) (\\S*) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the exact size of the words, we might want to match an exact number of characters (instead of using a star and matching _any number of characters_). It is possible to match an exact number of characters using another special syntax: putting a number between `{` and `}` immediately after some character in our regex. For example, in the lines of code below, `\\S{3}` means \"match exactly three non-space characters\", and the parenthesis surrounding it just put it in a group (so that it can be backreferenced in the replacement string):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat chases a dog\n",
      "the mouse eats the cheese\n"
     ]
    }
   ],
   "source": [
    "line1 = 'the dog chases a cat'        # <-- this line will change\n",
    "line3 = 'the mouse eats the cheese'   # <-- this line won't\n",
    "\n",
    "print(re.sub('(the|a) (\\S{3}) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line1))\n",
    "print(re.sub('(the|a) (\\S{3}) (.*) (the|a) (\\S*)', '\\\\1 \\\\5 \\\\3 \\\\4 \\\\2', line3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You despise not coconut'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"I not coconut\"\n",
    "re.sub(\"I (like){0,1}\", \"You despise \", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, since `mouse` is composed of 5 characters, the regular expression will be unable to match (the reason also has to do with the spaces surrounding the parenthesis), and thus the line won't change.\n",
    "\n",
    "These [quantifiers](https://www.regular-expressions.info/refrepeat.html) can also be used for groups. For example, the following regular expression transforms instances of `thethethe` or `aaa` into `the` and `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog chases a cat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('(the|a){3}', '\\\\1', 'thethethe dog chases aaa cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This syntax also allows you to specify a minimum and a maximum number of times when something might occur. The regex below fixes any number of repetitions of `the` and `a` between 2 and 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog chases a cat and the cat chases a mouse'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('(the|a){2,7}', '\\\\1',\n",
    "       'thethethethethe dog chases aaa cat and thethethethe cat chases aaaaaaa mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscellaneous\n",
    "---------------\n",
    "\n",
    "### Mixing Classes and Groups\n",
    "\n",
    "You should notice that classes and groups can of course be used together. If you want to match both `the` and `The`, you can always write something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog chases The cat and the cat chases the mouse'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('([Tt]he){2,7}', '\\\\1',\n",
    "       'ThethetheThethe dog chases theThe cat and theTheThethe cat chases the mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, use this kind of combination parsimoniously: as you can see, the first `the cat` got a capital `T`.\n",
    "\n",
    "### [Anchors](https://www.regular-expressions.info/refanchors.html)\n",
    "\n",
    "There may be cases when you'd like to make some replacement only if the matching happened in the beginning or the end of your input string. For example, you might want to change a `the` into `The` if it occurs in the beginning of the sentence, or also insert some punctuation in the end if it has none. Regex have two special characters for these tasks: a character that matches the beginning of the input string (`^`) and a character that matches its end (`$`).\n",
    "\n",
    "For example, if you'd like to capitalize the `the` in the beginning of your sentence, you might do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barks'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('^barks', 'The', 'the dog barks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and adding punctuation might be done with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that [^.!?] matches \"any character that is not \".\" nor \"!\" nor \"?\"\n",
    "re.sub('([^.!?])$', '\\\\1.', 'the dog barks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Theoretical Details\n",
    "-------------------------------\n",
    "\n",
    "The set of all languages that can be expressed by Regular Expressions is the set of the Regular Languages. Regular Languages are the simplest type of Formal Languages, but the theory of Formal Languages is not going to be discussed here. It is important to say, however, that they have a strong connection with the formalisms used in Generative Syntacticians. Further details about these topics can be found in:\n",
    "\n",
    "  * [Chomsky, N. (1956). Three models for the description of language](http://static.stevereads.com/papers_to_read/three_models_for_the_description_of_language.pdf)\n",
    "  * [Hopcroft et al. (2013). Introduction to Automata Theory, Languages, and Computation (3rd ed.)](https://en.wikipedia.org/wiki/Introduction_to_Automata_Theory,_Languages,_and_Computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful cleaning tips\n",
    "=============\n",
    "\n",
    "There is a ton of things you might want to do with your data to make it clean. Sometimes you might want to remove capitals, or you might want to transform all `&` into ` and `. You might to remove multiple spaces, or trailing spaces, or you might even want to remove hashtags, or even extract URLs (more in the end of the class), ... the list goes on and on.\n",
    "\n",
    "You have already learned how to transform `&` into ` and `. Let's write a regex for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Johnson  and  Johnson'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('&', ' and ', 'Johnson & Johnson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, now you have multiple spaces around the `and`. You can also clean this neatly with something we learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Johnson and Johnson'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\s+ matches 1 or more space characters\n",
    "re.sub('\\s+', ' ', 'Johnson  and  Johnson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This removes ALL multiple spaces in an input string. But let's say you only would like to remove either the left trailing spaces or the right trailing spaces. Python is your friend, offering the methods `lstrip()`, `rstrip()` and `strip()`. In the three examples below, notice how the multiple spaces surrounding the word `spaces` are not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminate   spaces  to the left    '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'    eliminate   spaces  to the left    '.lstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    eliminate   spaces   to the right'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'    eliminate   spaces   to the right    '.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eliminate   spaces   left and right'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'    eliminate   spaces   left and right    '.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be that whichever algorithm you want to process your text with does not \"like\" to differentiate between non-capitalized and capitalized first letters. Changing this can be easily done with `lower()`, `upper()` and `capitalize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like coconut with chocolate'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I like Coconut with Chocolate\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I LIKE COCONUT WITH CHOCOLATE'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I like Coconut with Chocolate\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like coconut with chocolate'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I like Coconut with Chocolate\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may also be useful if a given substring is present in a string. We had before done it with regular expressions in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found abc\n"
     ]
    }
   ],
   "source": [
    "my_string = 'my string now is aaaaaaaaabc'\n",
    "if re.search('abc', my_string):\n",
    "    print(\"Found abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that, if you are just looking for a small sequence of characters (without any of the special functionalities that regular expressions give you), you can do it in a much simpler way with the operator `in`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found abc\n"
     ]
    }
   ],
   "source": [
    "if 'abc' in my_string:\n",
    "    print(\"Found abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you might just want to remove the hashtags from your text. Doing that should be in your bloodstream at this point. All you need to do is using a regex to find them, and replace them with nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a  test with  '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('#[\\w]*', '', 'This is a #hashtag test with #ßŧrænge #cħæracters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[_Do notice, however, that hashtags do not seem to follow very well any consistent standard =/_](https://shkspr.mobi/blog/2010/02/hashtag-standards/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing with all we have learned\n",
    "=================\n",
    "\n",
    "Remember that the whole motivation for having learned about Regexes and the Python string utilities was that we wanted to perform Tokenization. We had assumed that it would be a good idea to consider special characters like `\".!?&,()[]/'` as separate tokens. It should be now clear that this kind of tokenization can now be easily performed using the tricks we learned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary',\n",
       " '&',\n",
       " 'Jane',\n",
       " 'knew',\n",
       " ',',\n",
       " 'despite',\n",
       " 'Bob',\n",
       " \"'\",\n",
       " 's',\n",
       " 'negation',\n",
       " ',',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " '(',\n",
       " 'cruelly',\n",
       " ')',\n",
       " 'killed',\n",
       " 'Elisa',\n",
       " '!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"Mary&Jane knew, despite Bob's negation, that he had (cruelly) killed Elisa!\"\n",
    "\n",
    "# 1. Separate special characters into separate words\n",
    "# (i.e., insert spaces around them)]\n",
    "line = re.sub('([&\\'*(),\\.!?/\"])', ' \\\\1 ', line)\n",
    "\n",
    "# 2. Remove multiple spaces (this is actually optional if you just call `.split()`, and\n",
    "# is done here only as an example)\n",
    "line = re.sub('\\s+', ' ', line)\n",
    "\n",
    "# 3. Remove trailing spaces\n",
    "line = line.strip()\n",
    "\n",
    "# 4. \"Tokenize\"\n",
    "line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally... notice, however, that even after all of our efforts, we still end up getting things like `Dr.` or `B.Sc.` \"wrong\" (or, at least, the result we get is not the most intuitive). Probably the best is to just use some library (although most of the libraries also get many things wrong anyway)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, how is Tokenization actually done?\n",
    "=====================================================\n",
    "\n",
    "This entire story was presented only to show the lots of things that Python offers for\n",
    "processing strings, both with the `string` class and with Regular Expressions.\n",
    "However, tokenization does not need to be implemented: the NLTK already gives you a\n",
    "function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'woman', \"'s\", 'coffee', 'was', '(', 'desperately', ',', 'quickly', ')', 'drunk', '.']\n"
     ]
    }
   ],
   "source": [
    "# If you still do not have the NLTK tokenizer installed, you might have to run\n",
    "# the following line to install it: (in that case, you'll need to uncomment it)\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Import the libraries for tokenizing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"The woman's coffee was (desperately, quickly) drunk.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last words on Tokenization\n",
    "==========================\n",
    "\n",
    "English is a \"well-behaved\" language in that it divides words mostly with spaces. Other\n",
    "languages, however, might not give any hint on where words start or end. For example,\n",
    "Chinese used not to use any indication of word boundaries. This style of writing is referred\n",
    "to as [_Scriptio continua_](https://en.wikipedia.org/wiki/Scriptio_continua). The examples\n",
    "below are from the Wikipedia article on the topic. The following example is from Chinese:\n",
    "\n",
    ">北京在中国北方；广州在中国南方。\n",
    "\n",
    ">北京在中国北方广州在中国南方\n",
    "\n",
    ">北京　   在　 中国　    北方   ； 广州　     在　 中国　    南方。\n",
    "\n",
    ">Běijīng zài Zhōngguó běifāng; Guǎngzhōu zài Zhōngguó nánfāng.\n",
    "\n",
    ">Beijing is in Northern China; Guangzhou is in Southern China.\n",
    "    \n",
    "\n",
    "Older Indo-European languages also used not to use any space between words. For example,\n",
    "both Latin and Greek used initially no spaces nor punctuation. The example below is from Latin:\n",
    "\n",
    ">NEQVEPORROQVISQVAMESTQVIDOLOREMIPSVMQVIADOLORSITAMETCONSECTETVRADIPISCIVELIT\n",
    "\n",
    ">NEQVE•PORRO•QVISQVAM•EST•QVI•DOLOREM•IPSVM•QVIA•DOLOR•SIT•AMET•CONSECTETVR•ADIPISCI•VELIT\n",
    "\n",
    ">Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\n",
    "\n",
    ">Nobody likes pain for its own sake, or looks for it and wants to have it, just because it is pain\n",
    "\n",
    "The internet lately has brought the topic back with hashtags and URLs like\n",
    "[gibtespommesheute.de](http://www.gibtesheutepommes.de/). Hashtag segmentation is an\n",
    "open research problem without a definitive solution, partly because of the limited\n",
    "pragmatic information computer systems have access to. A\n",
    "[funny example](http://cs.uccs.edu/~jkalita/work/reu/REU2015/FinalPapers/05Reuter.pdf)\n",
    "could be the hashtag #brainstorm, which could be segmented as\n",
    "\n",
    "    brainstorm --> bra in storm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Filtering\" (Preparing your data for Machine Learning)\n",
    "==================================\n",
    "\n",
    "I want to finish this class with some useful filtering tips that are likely to be useful to prepare your data for Machine Learning (ML is the topic of the next class).\n",
    "\n",
    "Removing URLs\n",
    "-------------\n",
    "\n",
    "Very often, Machine Learning algorithms expect to have some \"vocabulary\" of all types appearing in the corpus. If you have URLs in your corpus (for example, the corpus was collected from random places in the internet), you might end up with a vocabulary that is littered with things like _gibtespommesheute_, _localhost_ or even _pblandfort_ (which might or not be what you want).\n",
    "\n",
    "Identifying URLs is a little tricky: you probably already saw some programs identifying them wrong. Here, we will simply look for things like _http://_, _https://_, or _www._ in the beginning of any string. Importantly, the name of the protocol (HTTP, HTTPS, SFTP, ...) is normally a sequence of a few letters, followed by a colon and two slashes. We will also assume that the URL ends when we find a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to #URL# to look for it\n",
      "I don't like to buy in #URL# like you do\n",
      "I got a #URL# ... have you ever seen a url like this?\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Replaces URLs with #URL#\n",
    "#########\n",
    "\n",
    "line1 = \"I went to www.google.com to look for it\"\n",
    "line2 = \"I don't like to buy in http://amazon.com like you do\"\n",
    "line3 = \"I got a aptp://blah.blah.blah ... have you ever seen a url like this?\"\n",
    "\n",
    "line1 = re.sub('www\\.[\\S]*', '#URL#', line1)\n",
    "print(line1)\n",
    "\n",
    "line2 = re.sub('[\\w]+://[\\S]*', '#URL#', line2)\n",
    "line3 = re.sub('[\\w]+://[\\S]*', '#URL#', line3)\n",
    "print(line2)\n",
    "print(line3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML tags\n",
    "------------------\n",
    "\n",
    "Just like in the previous section, if you \"scraped\" your corpus automatically from the internet, you might have lots of HTML tags in your corpus that were accidentally taken in the process, which you likely want to clean out.\n",
    "\n",
    "Of course, there are several ways in which you can do this. We will do this in a very simple way, using a regex to eliminate anything that is in between a `<` and a `>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got a #TAG# MUCH #TAG# better car than yours...\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Replaces HTML tags with #TAG#\n",
    "#########\n",
    "\n",
    "line1 = \"I got a <b>MUCH</b> better car than yours...\"\n",
    "\n",
    "line1 = re.sub('<[^>]*>', ' #TAG# ', line1)\n",
    "\n",
    "# (remove multiple spaces)\n",
    "line1 = re.sub('\\s+', ' ', line1)\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that, if you have the characters `<` and `>` genuinely used in your data, then this will not only eliminate them, but also likely cause a lot of mess in your data.\n",
    "\n",
    "(this is just one example where it pays off a lot to think well what you are doing before start applying it to the entire corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting attributes in your HTML tags\n",
    "\n",
    "HTML tags sometimes contain attributes that are interpreted by your browser in several ways. A common example of a tag that normally appears with an attribute is the `<a>` tag. It normally appears with the attribute _src_ pointing to some other place, e.g.,\n",
    "\n",
    "    <a src=\"http://some_other_place.com\">\n",
    "\n",
    "Therefore, it might be that, instead of completely removing the tag, you would like to keep this information in some other variable. In the case of the `<a>` tag, you might want to keep the link as a metadata related to the tag. Let's define a function to do this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a   src=\"some_other_place.com\">\n",
      "src=\"some_other_place.com\"\n",
      "some_other_place.com\n",
      "Extracted link:  some_other_place.com\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Example of getting an attribute from a tag\n",
    "#########\n",
    "\n",
    "line1 = '<a   src=\"some_other_place.com\">Click here to go to some_other_place.com</a>'\n",
    "\n",
    "def get_url_from_src_attribute(line):\n",
    "    tag_regex = re.search('<[\\S]+\\s+src=\"[^\"]*\">', line1)\n",
    "    if not tag_regex:\n",
    "        return None\n",
    "    \n",
    "    # Now we have only the tag. We still need some filtering\n",
    "    print(tag_regex.group(0))\n",
    "    tag_str = tag_regex.group(0)\n",
    "\n",
    "    # Let's now remove only the attribute that we care about\n",
    "    attribute = re.search('src=\"[^\"]+\"', tag_str)\n",
    "    \n",
    "    # Notice that we are guaranteed to have the attribute, because we matched it\n",
    "    # in the previous regex\n",
    "    #if not attribute:\n",
    "    #    return None\n",
    "    \n",
    "    print(attribute.group(0))\n",
    "    attr_str = attribute.group(0)\n",
    "    \n",
    "    # Finally, we will take the part that we want\n",
    "    link = re.search('\"[^\"]+\"', attr_str).group(0)\n",
    "    link = link[1:-1]\n",
    "    print(link)\n",
    "    \n",
    "    return link\n",
    "\n",
    "extracted_link = get_url_from_src_attribute(line1)\n",
    "print(\"Extracted link: \", extracted_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing numbers\n",
    "-----------------\n",
    "\n",
    "Unless you are interested in the meanings of the sentences, it is often irrelevant which numbers are used in a sentence. For example, consider the sentences:\n",
    "\n",
    "    (1) She bought 50 packets of coffee last night.\n",
    "    (2) She bought 349 packets of coffee last night.\n",
    "    (3) She bought three packets of coffee last night.\n",
    "\n",
    "While in case 3 the number is written in letters, it is quite irrelevant how many packets of coffee (whatever this means) _she_ bought if your goal is, for example, to have an algorithm to learn the structure of the sentence: it is the same in all three cases. So if you want to train some algorithm to learn how to deal with numbers, you probably want to \"trick\" it into believing all numbers are \"the same word\".\n",
    "\n",
    "A normal way to do this is by replacing any numbers with some special symbol. This is easily done with regex (hopefully by now these ideas are already in your blood stream):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She bought ### packets of coffee; he bought ### packets of beer, and ### of chocolate\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Replaces any numbers into `###`\n",
    "#########\n",
    "\n",
    "line = \"She bought 349 packets of coffee; he bought 3 packets of beer, and 49 of chocolate\"\n",
    "\n",
    "line = re.sub('[0-9]+', '###', line)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran ###km yesterday.\n"
     ]
    }
   ],
   "source": [
    "# Notice that this will also take the numbers that are connected to works\n",
    "# (which might or not be what you want)\n",
    "\n",
    "line = \"I ran 5km yesterday.\"\n",
    "\n",
    "line = re.sub('[0-9]+', '###', line)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things for the next few classes\n",
    "===============================\n",
    "\n",
    "It would be awesome if you could have some packages already installed in your computer for the next class (to avoid having to solve problems in the class, when the time is limited)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Activate your virtual environment\n",
    "    # (if you are using Anaconda, you might not need this step, and you will probably want\n",
    "    # to install using the package manager that came along with it)\n",
    "    $ source /path/to/your/virtual/environment/bin/activate\n",
    "\n",
    "    # Install packages\n",
    "    # sklearn: contains several Machine Learning utility functions\n",
    "    # scipy and numpy: useful vector/matrix functions\n",
    "    # matplotlib: for showing plots in the screen\n",
    "    $ pip install sklearn scipy numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we are going to use a dataset for Sentiment Analysis, with ~5000 positive sentences, and ~5000 negative sentences.\n",
    "\n",
    "This dataset can be found [here](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples\n",
      "[\"a movie that will surely be profane , politically charged music to the ears of cho's fans .\", 'grant is certainly amusing , but the very hollowness of the character he plays keeps him at arms length', 'with \" ichi the killer \" , takashi miike , japan\\'s wildest filmmaker gives us a crime fighter carrying more emotional baggage than batman . . .', 'an intriguing cinematic omnibus and round-robin that occasionally is more interesting in concept than in execution .', 'the movie is full of fine performances , led by josef bierbichler as brecht and monica bleibtreu as helene weigel , his wife .']\n",
      "-----\n",
      "Negative samples\n",
      "[\"the sad thing about knockaround guys is its lame aspiration for grasping the coolness vibes when in fact the film isn't as flippant or slick as it thinks it is .\", '\" an entire film about researchers quietly reading dusty old letters . \"', \"it's hard to like a film about a guy who is utterly unlikeable , and shiner , starring michael caine as an aging british boxing promoter desperate for a taste of fame and fortune , is certainly that .\", 'while it is welcome to see a chinese film depict a homosexual relationship in a mature and frank fashion , lan yu never catches dramatic fire .', 'possibly the most irresponsible picture ever released by a major film studio .']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "positives = [sentence.strip() for sentence in open('rt-polaritydata/rt-polaritydata/rt-polarity.pos', 'r', encoding = 'latin1')]\n",
    "negatives = [sentence.strip() for sentence in open('rt-polaritydata/rt-polaritydata/rt-polarity.neg', 'r', encoding = 'latin1')]\n",
    "\n",
    "print(\"Positive samples\")\n",
    "print(random.sample(positives, 5))\n",
    "print(\"-----\")\n",
    "print(\"Negative samples\")\n",
    "print(random.sample(negatives, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1) Write the code to open the two datasets and tokenize the sentences. We will use the tokenized sentences from the next lecture on.\n",
    "\n",
    "2) Create a vocabulary for the dataset. That is, use nltk.FreqDist() to create a list of all _types_ (the different tokens) in the dataset. A _vocabulary_ is basically the set of all tokens present in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
