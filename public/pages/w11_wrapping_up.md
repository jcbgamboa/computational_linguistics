---
layout: page
---


W11 Wrapping Up
===============

I prepared a "W11 Wrapping Up.ipynb" file containing the content of this week.
The first two "parts" are presented in the videos I uploaded.

As you know, in this week I had planned to speak a little about Language Models.
I felt, however, that everything that I had to say was already said, and
whatever was "left" was only going to be a lot of math, that I didn't feel would
be useful for you. So I changed my mind and created this "Wrapping Up" content.
The goals are the following:

* To explore a little more these word vectors (that everyone keeps using and
  talking about these days)
* To show you one way that you can just "access" the word vectors, without
  having to train a Machine Learning model, or deal with too many complications.
  In our case, we will use a Python library called Flair.
* To show you some of the "limitations" of these vectors, and try to help you
  get some intuition of what they are doing.

Because this class is so "unorthodox", almost none of this will appear in the
exam. There are only two things that I want you to know from this class (that
may appear in the exam):

* Why BERT vectors are called "contextual" word vectors.
* Word vectors do not represent synonymy (at least not in the way we are used
  to thing about it)

I hope this course was useful. Thank you for your participation.

<iframe width="560" height="315" src="https://www.youtube.com/embed/WGakSaqvo2Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Eq2JUSnWzOs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

*NOTICE THAT* there is more content in the .ipynb file than in the videos!


Where to go from here?
----------------------

The techniques we explored this week are quite new, and still "hot" in the
NLP literature. If you are more interested in them, then you should probably
take a look at the
[Stanford course on Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).


